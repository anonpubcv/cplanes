<!DOCTYPE html>
<html lang="en">
<head>
  <title>cplanes</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/css/bootstrap.min.css" rel="stylesheet">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.2/dist/js/bootstrap.bundle.min.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.7.1/jquery.min.js"></script>
  <link rel="stylesheet" href="css/range.css">
  <link rel="stylesheet" href="css/styles.css">
</head>
<body>

    <!--<div class="container-lg d-flex justify-content-center align-items-center"></div>-->
    <div class="container-xl">
        <div class="container-fluid p-3 bg-light text-white text-center">
            <h2 class="fw-bolder font-black">Conditional Plane-based Scene Agnostic Representation </h2> 
            <h2 class="fw-bolder font-black">for Novel View Synthesis</h2>
        </div>
        </br>
        <div class="text-center">
            <h2 class="fw-bold">Abstract</h2> 
        </div>
        <div class="container p-3 bg-light">
            <p class="text-center">Existing explicit and hybrid neural representations for novel view synthesis are scene-specific. In other words, they represent only a single scene and require retraining for every novel scene. Implicit scene-agnostic methods rely on large and deep MLPs that are conditioned on learned features. They are computationally expensive at training and rendering times. In contrast, we propose a novel hybrid representation that learns to represent multiple static and dynamic scenes during training, and renders per-scene novel views during inference. The method consists of a deformation network, explicit feature planes, and a conditional decoder. The explicit feature planes are used to represent a time-stamped view-space volume and a shared canonical volume across multiple scenes.  The conditional decoder estimates the color and density for each scene constrained on a scene-specific latent vector.  We evaluate and compare the performance of the proposed representation on static (NeRF) and dynamic (Plenoptic videos) datasets. Our results show that explicit planes combined with tiny MLPs can efficiently train multiple scenes simultaneously.</p> 
        </div>

        </br>

        <div class="container-fluid p-3 " style="text-align:center">
            <div class="row">
                <div class="col-sm-2">
                    <div class="text-center">
                        <video width="320" height="240" autoplay muted loop>
                            <source src="videos/static/static_lego_0.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <!--<div class="col-sm-2">
                    <div class="text-center">
                        <video width="320" height="240" autoplay muted loop>
                            <source src="videos/static/static_hotdog_1.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>-->
                <div class="col-sm-2">
                    <div class="text-center">
                        <video width="320" height="240" autoplay muted loop>
                            <source src="videos/static/static_chair_2.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <div class="col-sm-2">
                    <div class="text-center">
                        <video width="320" height="240" autoplay muted loop>
                            <source src="videos/static/static_mic_3.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>

                <div class="col-sm-2">
                    <div class="text-center">
                        <video width="320" height="240" autoplay muted loop>
                            <source src="videos/dynamic synthetic/0_mutant.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <div class="col-sm-2">
                    <div class="text-center">
                        <video width="320" height="240" autoplay muted loop>
                            <source src="videos/dynamic synthetic/1_standup.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <div class="col-sm-2">
                    <div class="text-center">
                        <video width="320" height="240" autoplay muted loop>
                            <source src="videos/dynamic synthetic/3_trex.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
            
            <div class="text-start">
                <p>Images rendered by the proposed method for NeRF (static) and D-NeRF (dynamic) datasets after training simultaneously with multiple scenes.</p>
                </br>
            </div>
        </div>

        <div class="container-fluid p-3 " style="text-align:center">
            
            <span id="valBox"></span>
            <img id="slideImages" src="default-image.png" width="250" height="250">
            </br>
            <div data-tip="Slide through scene dimention">
                <input id="selector" style="width: 30%;" type="range" min="1" max="40" step="1"> 
            </div>
            <p>Scene Dimention</p>
            
            <div class="text-start">
                <p>Visualization of multi-scene representation. The slider includes the novel views rendered by the proposed representation for a fixed camera pose with different auto-decoded latents.</p>
                </br>
            </div>
        </div>

        <div class="container-fluid p-3">
            <div class="row">
                <div class="col-sm-4">
                    <div class="text-center">
                        <img src="images/overview_img.png" class="img-rounded" alt="No Image" width="1250" height="430"> 
                    </div>
                </div>
            </div>

            <div class="text-start">
                <p>The method consists of four components: (1) a shared canonical space representation, (2) a deformation network, (3) a view-space representation, and (4) a conditional MLP decoder.</p>
                </br>
            </div>
        </div>

        <div class="container-fluid p-3 " style="text-align:center">
            <div class="row">           
                <div class="col-sm-3">
                    <div class="text-center">
                        <video width="320" height="240" autoplay muted loop>
                            <source src="videos/dynamic/coffee_martini.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>

                <div class="col-sm-3">
                    <div class="text-center">
                        <video width="320" height="240" autoplay muted loop>
                            <source src="videos/dynamic/flame_salmon.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <div class="col-sm-3">
                    <div class="text-center">
                        <video width="320" height="240" autoplay muted loop>
                            <source src="videos/dynamic/sear_steak_test.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
                <div class="col-sm-3">
                    <div class="text-center">
                        <video width="320" height="240" autoplay muted loop>
                            <source src="videos/dynamic/cook_spinach.mp4" type="video/mp4">
                        </video>
                    </div>
                </div>
            </div>
            
            <div class="text-start">
                <p>Novel views synthesized for Plenoptic video dataset.</p>
                </br>
            </div>
        </div>
    </div>

    <script>
        $( document ).ready(function() {
            imageElement.src = "sequence/1_test028.png";
            $('#selector').val(1);
        });

        const imageElement = document.getElementById("slideImages");
        /*const selectorElement = document.getElementById("selector");
        selectorElement.onchange = () => {
        const step = selectorElement.valueAsNumber;  
            console.log(step);
        for (let i = 20; i >= 1; i--) {
            console.log(i);
            if (i === step) {
                console.log("sequence/" + i + "_test028.png");
                imageElement.src = "sequence/" + i + "_test028.png";
            }  
        }*/
        
        $(document).on('input', '#selector', function() {
            var step = $(this).val() ;
            for (let i = 1; i <= 40; i++) {
                if (i == step) {
                    imageElement.src = "sequence/" + i + "_test028.png";
                }  
            }
        });
        
        
    </script>
</body>
</html>
